{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import WordPunctTokenizer\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models import KeyedVectors\n",
    "# import tensorflow as tf\n",
    "from tqdm import tqdm\n",
    "from torch.utils import data\n",
    "import nltk\n",
    "import re\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import pickle\n",
    "import torch\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import torch.nn.functional as F\n",
    "from sklearn import metrics\n",
    "from torch.utils.data import DataLoader\n",
    "cache_dir = 'cache'\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install nltk\n",
    "!pip install gensim\n",
    "!pip install python-Levenshtein\n",
    "!pip install tensorflow-gpu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Config(object):\n",
    "    def __init__(self):\n",
    "        self.train_all = \"data/pseudo_concat_train.csv\"\n",
    "        # 合并title和abstract的csv\n",
    "        self.train_path = \"predata/train_x.csv\"\n",
    "        self.valid_path = \"predata/valid_x.csv\"\n",
    "        self.testall_path = \"data/test.csv\"\n",
    "        self.test_path = \"predata/test_x.csv\"\n",
    "        # 词典\n",
    "        self.vocab_path = \"data/vocab.pkl\"\n",
    "        # label的类别\n",
    "        self.label_path = \"predata/label_id2cate.pkl\"\n",
    "\n",
    "        self.process_trainset_path = \"predata/train_set.npy\"\n",
    "        self.process_trainlabel_path = \"predata/train_label.npy\"\n",
    "        self.process_testset_path = \"predata/test_set.npy\"\n",
    "        self.fastText_path = \"model/fasttext.bin\"\n",
    "        self.word2vec_path = \"model/word2vec.bin\"\n",
    "        self.glove_path  = \"model/glove.bin\"\n",
    "        # 嵌入长度\n",
    "        self.embedding_size = 128\n",
    "        # 词典最大词数\n",
    "        self.max_vocab_size = 50000\n",
    "        # 词向量维度\n",
    "        self.max_len = 128\n",
    "        # 分类类别数\n",
    "        self.num_class = 39\n",
    "        # 模型保存路径\n",
    "        self.save_path = \"saved/\"\n",
    "        self.batch_size = 1000\n",
    "        self.lr = 0.001\n",
    "        self.num_epochs = 8# 50\n",
    "        self.model = \"TextCNN\"\n",
    "args = Config()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 TextCNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextCNN(nn.Module):\n",
    "    def __init__(self, args, pretrained_path):\n",
    "        super(TextCNN, self).__init__()\n",
    "        self.dim_embed = args.embedding_size\n",
    "        self.dropout = 0.4\n",
    "        self.num_filters = 256\n",
    "        self.kernel_size = (4, 5, 3)\n",
    "        self.max_len = args.max_len\n",
    "        self.n_vocab = pretrained_path.shape[0]  # 不使用预训练词向量时的词典长度\n",
    "        self.num_classes = args.num_class# f分类类别数\n",
    "        self.pretrained = True\n",
    "        self.pretrained_path = pretrained_path\n",
    "\n",
    "        if self.pretrained: \n",
    "            self.embedding = nn.Embedding.from_pretrained(self.pretrained_path, freeze=False)\n",
    "        else:\n",
    "            self.embedding = nn.Embedding(self.n_vocab, self.dim_embed, padding_idx=self.n_vocab - 1)\n",
    "\n",
    "        self.conv1 = nn.Conv2d(1, self.num_filters, (self.kernel_size[0], self.dim_embed))\n",
    "        self.conv2 = nn.Conv2d(1, self.num_filters, (self.kernel_size[1], self.dim_embed))\n",
    "        self.conv3 = nn.Conv2d(1, self.num_filters, (self.kernel_size[2], self.dim_embed))\n",
    "        self.max_pool1 = nn.MaxPool2d((self.max_len - self.kernel_size[0] + 1, 1))\n",
    "        self.max_pool2 = nn.MaxPool2d((self.max_len - self.kernel_size[1] + 1, 1))\n",
    "        self.max_pool3 = nn.MaxPool2d((self.max_len - self.kernel_size[1] + 1, 1))\n",
    "        self.dropout = nn.Dropout(self.dropout)\n",
    "        self.fc = nn.Linear(self.num_filters * 3, self.num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size = x.shape[0]\n",
    "\n",
    "        x = self.embedding(x)              # [batch_size, max_len, dim_embed]\n",
    "        x = x.unsqueeze(1)                 # [batch_size, 1, max_len, dim_embed]\n",
    "\n",
    "        x1 = F.relu(self.conv1(x))         # [batch_size, num_filters, max_len-kernel_size[0], 1]\n",
    "        x2 = F.relu(self.conv2(x))         # [batch_size, num_filters, max_len-kernel_size[1], 1]\n",
    "        x3 = F.relu(self.conv3(x))         # [batch_size, num_filters, max_len-kernel_size[2], 1]\n",
    "\n",
    "        x1 = self.max_pool1(x1)            # [batch_size, num_filters, 1, 1]\n",
    "        x2 = self.max_pool2(x2)            # [batch_size, num_filters, 1, 1]\n",
    "        x3 = self.max_pool3(x3)            # [batch_size, num_filters, 1, 1]\n",
    "\n",
    "        x = torch.cat((x1, x2, x3), -1)    # [batch_size, num_filters, 1, 3]\n",
    "        x = x.view(batch_size, 1, -1)      # [batch_size, 1, num_filters*3]\n",
    "\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc(x)                     # [batch_size, 1, 2]\n",
    "        x = x.view(-1, self.num_classes)   # [batch_size, 2]\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 FastText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FastText(nn.Module):\n",
    "    def __init__(self, args, pretrained_path):\n",
    "        super(FastText, self).__init__()\n",
    "        self.dim_embed = args.embedding_size\n",
    "        self.hidden_size = 256\n",
    "        self.n_vocab = pretrained_path.shape[0]\n",
    "        self.num_classes = args.num_class\n",
    "        self.pretrained = True\n",
    "        self.pretrained_path = pretrained_path\n",
    "        # 在文本分类任务中，0.6精度最高\n",
    "        self.dropout = 0.6\n",
    "        if self.pretrained:\n",
    "            self.embedding = nn.Embedding.from_pretrained(\n",
    "                self.pretrained_path, freeze=False)\n",
    "        else:\n",
    "            self.embedding = nn.Embedding(self.n_vocab, self.dim_embed)\n",
    "\n",
    "        self.dropout = nn.Dropout(self.dropout)\n",
    "        self.fc1 = nn.Linear(self.dim_embed, self.hidden_size)\n",
    "        self.fc2 = nn.Linear(self.hidden_size, self.num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size = x.shape[0]\n",
    "\n",
    "        x = self.embedding(x)\n",
    "        x = x.mean(dim=1)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 TextRNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextRNN(nn.Module):\n",
    "    def __init__(self, args, pretrained_path):\n",
    "        super(TextRNN, self).__init__()\n",
    "        self.pretrained = True\n",
    "        self.pretrained_path = pretrained_path\n",
    "        self.n_vocab = pretrained_path.shape[0]\n",
    "        self.dim_embed = args.embedding_size\n",
    "        self.hidden_size = 64\n",
    "        self.num_layers = 2\n",
    "        self.dropout = 0.4\n",
    "        self.num_classes = args.num_class\n",
    "\n",
    "        if self.pretrained:\n",
    "            self.embedding = nn.Embedding.from_pretrained(\n",
    "                self.pretrained_path, freeze=False)\n",
    "        else:\n",
    "            self.embedding = nn.Embedding(\n",
    "                self.n_vocab, self.dim_embed, padding_idx=self.n_vocab - 1)\n",
    "\n",
    "        self.lstm = nn.LSTM(self.dim_embed, self.hidden_size, self.num_layers,\n",
    "                            bidirectional=True, batch_first=True, dropout=self.dropout)\n",
    "        self.fc = nn.Linear(self.hidden_size * 2, self.num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        x, _ = self.lstm(x)\n",
    "        x = self.fc(x[:, -1, :])\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 DPCNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DPCNN(nn.Module):\n",
    "    def __init__(self, args, pretrained_path):\n",
    "        super(DPCNN, self).__init__()\n",
    "        self.dim_embed = args.embedding_size\n",
    "        self.num_filters = 256\n",
    "        self.kernel_size = 3\n",
    "        self.n_vocab = pretrained_path.shape[0]\n",
    "        self.num_classes = args.num_class\n",
    "        self.pretrained = True\n",
    "        self.pretrained_path = pretrained_path\n",
    "\n",
    "        if self.pretrained: \n",
    "            self.embedding = nn.Embedding.from_pretrained(self.pretrained_path, freeze=False)\n",
    "        else:\n",
    "            self.embedding = nn.Embedding(self.n_vocab, self.dim_embed)\n",
    "\n",
    "        self.conv_region = nn.Conv2d(1, self.num_filters, (self.kernel_size, self.dim_embed), stride=1)\n",
    "        self.conv = nn.Conv2d(self.num_filters, self.num_filters, (self.kernel_size, 1), stride=1)\n",
    "        self.max_pool = nn.MaxPool2d(kernel_size=(self.kernel_size, 1), stride=2)\n",
    "        self.padding1 = nn.ZeroPad2d((0, 0, 1, 1))  # top bottom\n",
    "        self.padding2 = nn.ZeroPad2d((0, 0, 0, 1))  # bottom\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc = nn.Linear(self.num_filters, self.num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)     # [batch_size, max_len, dim_embed]\n",
    "        x = x.unsqueeze(1)        # [batch_size, 1, max_len, dim_embed]\n",
    "        x = self.conv_region(x)   # [batch_size, num_filters, max_len-kernel_size, 1]\n",
    "        x = self.padding1(x)      # [batch_size, num_filters, max_len, 1]\n",
    "        x = self.relu(x)\n",
    "        x = self.conv(x)          # [batch_size, num_filters, max_len-kernel_size, 1]\n",
    "        x = self.padding1(x)      # [batch_size, num_filters, max_len, 1]\n",
    "        x = self.relu(x)\n",
    "        x = self.conv(x)          # [batch_size, num_filters, max_len-kernel_size, 1]\n",
    "        while x.size()[2] > 2:\n",
    "            x = self._block(x)    # [batch_size, num_filters, 1, 1]\n",
    "        x = x.squeeze()           # [batch_size, num_filters]\n",
    "        x = self.fc(x)            # [batch_size, num_classes]\n",
    "\n",
    "        return x\n",
    "\n",
    "    def _block(self, x):\n",
    "        x = self.padding2(x)\n",
    "        px = self.max_pool(x)\n",
    "\n",
    "        x = self.padding1(px)\n",
    "        x = self.relu(x)\n",
    "        x = self.conv(x)\n",
    "\n",
    "        x = self.padding1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.conv(x)\n",
    "\n",
    "        # Short Cut\n",
    "        x = x + px\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5 对抗训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#对抗训练\n",
    "\n",
    "class PGD():\n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "        self.emb_backup = {}\n",
    "        self.grad_backup = {}\n",
    "        \n",
    "\n",
    "    def attack(self, epsilon=1., alpha=0.3, emb_name='emb', is_first_attack=1):\n",
    "        # emb_name这个参数要换成你模型中embedding的参数名\n",
    "        for name, param in self.model.named_parameters():\n",
    "            \n",
    "            if param.requires_grad and emb_name in name:\n",
    "                if is_first_attack==0:\n",
    "                    self.emb_backup[name] = param.data.clone()\n",
    "                   \n",
    "                    \n",
    "                norm = torch.norm(param.grad)\n",
    "                if norm != 0 and not torch.isnan(norm):\n",
    "                    r_at = alpha * param.grad / norm\n",
    "                    param.data.add_(r_at)\n",
    "                    param.data = self.project(self.emb_backup,name, param.data, epsilon)\n",
    "\n",
    "    def restore(self, emb_name='emb'):\n",
    "        # emb_name这个参数要换成你模型中embedding的参数名\n",
    "        for name, param in self.model.named_parameters():\n",
    "            if param.requires_grad and emb_name in name: \n",
    "                if len(self.emb_backup)==0:\n",
    "                    continue\n",
    "                assert name in self.emb_backup\n",
    "                param.data = self.emb_backup[name]\n",
    "        self.emb_backup = {}\n",
    "\n",
    "    def project(self,t, param_name, param_data, epsilon):\n",
    "        #assert param_name in self.emb_backup\n",
    "        if param_name not in self.emb_backup:\n",
    "            return param_data\n",
    "       \n",
    "        r = param_data - t[param_name]\n",
    "        if torch.norm(r) > epsilon:\n",
    "            r = epsilon * r / torch.norm(r)\n",
    "        return t[param_name] + r\n",
    "\n",
    "    def backup_grad(self):\n",
    "        for name, param in self.model.named_parameters():\n",
    "            if param.requires_grad:\n",
    "                self.grad_backup[name] = param.grad.clone()\n",
    "\n",
    "    def restore_grad(self):\n",
    "        for name, param in self.model.named_parameters():\n",
    "            if len(self.grad_backup)==0:\n",
    "                continue\n",
    "            if param.requires_grad:\n",
    "                param.grad = self.grad_backup[name]\n",
    "class FGM():\n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "        self.backup = {}\n",
    "        print(model.named_parameters())\n",
    "\n",
    "    def attack(self, epsilon=1., emb_name='embedding'):\n",
    "        # emb_name这个参数要换成你模型中embedding的参数名\n",
    "        for name, param in self.model.named_parameters():\n",
    "            if param.requires_grad and emb_name in name:\n",
    "                self.backup[name] = param.data.clone()\n",
    "                norm = torch.norm(param.grad)\n",
    "                if norm != 0 and not torch.isnan(norm):\n",
    "                    r_at = epsilon * param.grad / norm\n",
    "                    param.data.add_(r_at)\n",
    "\n",
    "    def restore(self, emb_name='embedding'):\n",
    "        # emb_name这个参数要换成你模型中embedding的参数名\n",
    "        for name, param in self.model.named_parameters():\n",
    "            if param.requires_grad and emb_name in name: \n",
    "                assert name in self.backup\n",
    "                param.data = self.backup[name]\n",
    "        self.backup = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 训练集预处理和封装"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 去除停用词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import re\n",
    "import nltk\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch.utils import data\n",
    "from sklearn.utils import resample\n",
    "\n",
    "# en_stop = set(STOP_WORDS)\n",
    "\n",
    "# custom_stop_words = [\n",
    "#     'doi', 'preprint', 'copyright', 'peer', 'reviewed', 'org', 'https', 'et', 'al', 'author', 'figure',\n",
    "#     'rights', 'reserved', 'permission', 'used', 'using', 'biorxiv', 'medrxiv', 'license', 'fig', 'fig.',\n",
    "#     'al.', 'elsevier', 'pmc', 'czi', 'www'\n",
    "# ]\n",
    "# for word in custom_stop_words:\n",
    "#     en_stop.add(word)\n",
    "\n",
    "en_stop = [\n",
    "    'doi', 'preprint', 'copyright', 'peer', 'reviewed', 'org', 'https', 'et', 'al', 'author', 'figure',\n",
    "    'rights', 'reserved', 'permission', 'used', 'using', 'biorxiv', 'medrxiv', 'license', 'fig', 'fig.',\n",
    "    'al.', 'elsevier', 'pmc', 'czi', 'www','i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves',\n",
    "    'you', 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', 'her', 'hers',\n",
    "    'herself', 'it', 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which',\n",
    "    'who', 'whom', 'this', 'that', 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being',\n",
    "    'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if',\n",
    "    'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against',\n",
    "    'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up',\n",
    "    'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here',\n",
    "    'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most',\n",
    "    'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very',\n",
    "    's', 't', 'can', 'will', 'just', 'don', 'should', 'now', 'd', 'll', 'm', 'o', 're', 've', 'y',\n",
    "    'ain', 'aren', 'couldn', 'didn', 'doesn', 'hadn', 'hasn', 'haven', 'isn', 'ma', 'mightn', 'mustn', 'needn', 'shan', 'shouldn', 'wasn', 'weren', 'won', 'wouldn'\n",
    "]\n",
    "def preprocess_text(document):\n",
    "    stemmer = WordNetLemmatizer()\n",
    "\n",
    "    document = str(document)\n",
    "    document = document.replace(\"\\n\", ' ')\n",
    "    document = document.replace(\"/'\", '')\n",
    "\n",
    "    document = re.sub(r'\\W', ' ', document)\n",
    "\n",
    "    # 删除所有单个字符\n",
    "    document = re.sub(r'\\s+[a-zA-Z]\\s+', ' ', document)\n",
    "\n",
    "    # 从开头删除单个字符\n",
    "    document = re.sub(r'\\^[a-zA-Z]\\s+', ' ', document)\n",
    "\n",
    "    # 用单个空格替换多个空格\n",
    "    document = re.sub(r'\\s+', ' ', document, flags=re.I)\n",
    "\n",
    "    # 删除前缀“b”\n",
    "    # document = re.sub(r'^b\\s+', '', document)\n",
    "\n",
    "    # 数字泛化：，所有大于9的数字都被hashs替换了。即成为# #,123变成# # #或15.80€变成# #,# #€。\n",
    "    document = re.sub('[0-9]{5,}', '#####', document)\n",
    "    document = re.sub('[0-9]{4}', '####', document)\n",
    "    document = re.sub('[0-9]{3}', '###', document)\n",
    "    document = re.sub('[0-9]{2}', '##', document)\n",
    "    # 转换为小写\n",
    "    document = document.lower()\n",
    "\n",
    "    # 词形还原\n",
    "    tokens = document.split()\n",
    "    tokens = [stemmer.lemmatize(word) for word in tokens]\n",
    "    # 去停用词\n",
    "    tokens = [word for word in tokens if word not in en_stop]\n",
    "    # 去低频词\n",
    "    tokens = [word for word in tokens if len(word) > 3]\n",
    "\n",
    "    preprocessed_text = ' '.join(tokens)\n",
    "\n",
    "    return preprocessed_text\n",
    "    return document\n",
    "\n",
    "\n",
    "def load_data_kfold(dataset,batch_size, k, n):\n",
    "    print(\"Stacking第{}折正在划分数据集\".format(n+1))\n",
    "\n",
    "    l = len(dataset)\n",
    "    print(l)\n",
    "    shuffle_dataset = True\n",
    "    random_seed = 42  # fixed random seed\n",
    "    indices = list(range(l))\n",
    "\n",
    "    if shuffle_dataset:\n",
    "        np.random.seed(random_seed)\n",
    "        np.random.shuffle(indices)  # shuffle\n",
    "    # Collect indexes of samples for validation set.\n",
    "    val_indices = indices[int(l / k) * n:int(l / k) * (n + 1)]\n",
    "    train_indices = list(set(indices).difference(set(val_indices)))\n",
    "    train_sampler = data.SubsetRandomSampler(train_indices)  # build Sampler\n",
    "    valid_sampler = data.SubsetRandomSampler(val_indices)\n",
    "    train_loader = data.DataLoader(dataset, batch_size=batch_size,\n",
    "                                               sampler=train_sampler)  # build dataloader for train set\n",
    "    validation_loader = data.DataLoader(dataset, batch_size=batch_size,\n",
    "                                                    sampler=valid_sampler)  # build dataloader for validate set\n",
    "    print(\"划分完成\")\n",
    "    return train_loader, validation_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 训练词向量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle as pkl\n",
    "from torch.utils import data\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from nltk import WordPunctTokenizer\n",
    "import tensorflow as tf\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim.models import Word2Vec\n",
    "import torch\n",
    "from nltk import WordPunctTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from torch.utils import data\n",
    "from tqdm import tqdm\n",
    "# import tensorflow as tf\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models import FastText\n",
    "import torch\n",
    "from glove import Glove\n",
    "from glove import Corpus\n",
    "from nltk import WordPunctTokenizer\n",
    "def build_word2vec(args,train):\n",
    "    trainall_title = list(train['title'])\n",
    "    trainall_abstract = list(train['abstract'])\n",
    "    trainall_combine = np.empty_like(trainall_title)\n",
    "    for i in range(len(trainall_title)):\n",
    "        trainall_combine[i] = trainall_title[i] + ' <sep> ' + trainall_abstract[i]\n",
    "    # Prepare FastText Training Data\n",
    "    print('构造词向量：删除不需要字符...')\n",
    "    final_corpus = [preprocess_text(sentence) for sentence in trainall_combine if sentence.strip() != '']\n",
    "    print('构造词向量：分词...')\n",
    "    word_punctuation_tokenizer = WordPunctTokenizer()\n",
    "    word_tokenized_corpus = [word_punctuation_tokenizer.tokenize(sent) for sent in final_corpus]\n",
    "    # 选择单词编码工具\n",
    "    tokenizer = tf.keras.preprocessing.text.Tokenizer(lower=True)\n",
    "    tokenizer.fit_on_texts(word_tokenized_corpus)\n",
    "\n",
    "    # Train a FastText Model:\n",
    "    # embedding_size = 128\n",
    "    window_size = 40\n",
    "    min_word = 5\n",
    "    down_sampling = 1e-2\n",
    "    vector_tag = 'fasttext'\n",
    "    print('构造词典：训练词向量...')\n",
    "    # 选择Fasttext词向量\n",
    "    if vector_tag =='fasttext':\n",
    "        if os.path.exists(args.fastText_path):\n",
    "            ft_model = FastText.load(args.fastText_path)\n",
    "        else:\n",
    "            print(\"正在训练fasttext词向量\")\n",
    "            ft_model = FastText(word_tokenized_corpus,\n",
    "                                vector_size=args.embedding_size,\n",
    "                                window=window_size,\n",
    "                                min_count=min_word,\n",
    "                                sample=down_sampling,\n",
    "                                sg=1,\n",
    "                                epochs=40)\n",
    "            #save your model as\n",
    "            print(\"word2vec\")\n",
    "            wv_model=Word2Vec(word_tokenized_corpus, vector_size=args.embedding_size, min_count=5, epochs=50)\n",
    "            print(\"glove\")\n",
    "    elif vector_tag =='glove':\n",
    "        if os.path.exists(args.glove_path):\n",
    "            corpus_model = Corpus()\n",
    "            corpus_model.fit(word_tokenized_corpus, window=5)\n",
    "            gl_model = Glove(no_components=args.embedding_size, learning_rate=0.05)\n",
    "            gl_model.fit(corpus_model.matrix, epochs=20,no_threads=1, verbose=True)\n",
    "            gl_model.add_dictionary(corpus_model.dictionary)\n",
    "            gl_model.save('glove.bin')\n",
    "         else:\n",
    "           gl_model = Glove.load(args.glove_path)\n",
    "    elif vector_tag =='word2vec':\n",
    "        if os.path.exists(args.word2vec_path):\n",
    "            ft_model = KeyedVectors.load_word2vec_format(args.word2vec_path, binary=True)\n",
    "        else:\n",
    "            print(\"正在训练word2vec词向量\")\n",
    "            ft_model = Word2Vec(\n",
    "                word_tokenized_corpus, size=args.embedding_size, min_count=min_word, sg=1, iter=20)\n",
    "            ft_model.wv.save_word2vec_format(args.word2vec_path, binary=True)\n",
    "    # Extract fasttext learned embedding and put them in a numpy array 初始化空的嵌入矩阵\n",
    "    embedding_matrix_ft1 = np.random.random(\n",
    "        (len(tokenizer.word_index) + 1,args.embedding_size))\n",
    "    embedding_matrix_ft2 = np.random.random(\n",
    "        (len(tokenizer.word_index) + 1,args.embedding_size))\n",
    "    embedding_matrix_ft3 = np.random.random(\n",
    "        (len(tokenizer.word_index) + 1, args.embedding_size))\n",
    "    pas = 0\n",
    "    # 预训练矩阵\n",
    "    for word, i in tokenizer.word_index.items():\n",
    "\n",
    "        try:\n",
    "            #embedding_matrix_ft1[i] = ft_model.wv[word]\n",
    "            embedding_matrix_ft2[i] = wv_model.wv[word]\n",
    "           # embedding_matrix_ft3[i] = gl_model.word_vectors[model.dictionary[word]]\n",
    "        except:\n",
    "            pas += 1\n",
    "    # 如果需要，选择合并三种词向量，就去掉此处注释\n",
    "    # embedding_matrix_ft=np.concatenate([embedding_matrix_ft1, embedding_matrix_ft2, embedding_matrix_ft3], axis=1)\n",
    "    print(embedding_matrix_ft.shape)\n",
    "    # 只选择Fasttext的词向量矩阵\n",
    "    # return embedding_matrix_ft1, tokenizer\n",
    "    # 只选择word2vec的词向量矩阵\n",
    "    return embedding_matrix_ft2, tokenizer\n",
    "    # 只选择glove的词向量\n",
    "    # return embedding_matrix_ft3, tokenizer\n",
    "    # 选择合并三种词向量的矩阵\n",
    "    # return embedding_matrix_ft, tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 封装训练集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PaperData(data.Dataset):\n",
    "    def __init__(self, args, tokenizer,split='train'):\n",
    "        self.texts = []\n",
    "        self.labels = []\n",
    "        self.args = args\n",
    "        self.split = split\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "        text_tokenizer = WordPunctTokenizer()\n",
    "        if self.split == \"train\":\n",
    "            print(\"训练集预处理...\")\n",
    "            if not os.path.exists(args.process_trainset_path):\n",
    "                train = pd.read_csv(self.args.train_path)\n",
    "                word_tokenized_corpus = []\n",
    "                for text in tqdm(train['text']):\n",
    "                    textp = preprocess_text(text)\n",
    "                    tokentext = text_tokenizer.tokenize(textp)\n",
    "                    word_tokenized_corpus.append(tokentext)\n",
    "\n",
    "                print('训练集预处理：分词...')\n",
    "                sequence_train = tokenizer.texts_to_sequences(word_tokenized_corpus)\n",
    "                sequence_train = tf.keras.preprocessing.sequence.pad_sequences(sequence_train, maxlen=args.embedding_size)\n",
    "\n",
    "                self.texts = sequence_train\n",
    "                self.labels = list(train['label'])\n",
    "                np.save(args.process_trainset_path,sequence_train)\n",
    "                np.save(args.process_trainlabel_path, np.array(self.labels))\n",
    "            else:\n",
    "                train_set = np.load(args.process_trainset_path)\n",
    "                train_label = np.load(args.process_trainlabel_path)\n",
    "                self.texts = list(train_set)\n",
    "                self.labels = list(train_label)\n",
    "        elif self.split == \"test\":\n",
    "            print(\"测试集预处理...\")\n",
    "            if not os.path.exists(args.process_testset_path):\n",
    "                test = pd.read_csv(self.args.test_path, sep='\\t')\n",
    "                # 拼接title与abstract\n",
    "                # test['text'] = test['title'] + ' ' + test['abstract']\n",
    "                word_tokenized_corpus = []\n",
    "                # for i, text in test['text'].items():\n",
    "                for text in tqdm(test['text']):\n",
    "                    textp = preprocess_text(text)\n",
    "                    tokentext = text_tokenizer.tokenize(textp)\n",
    "                    word_tokenized_corpus.append(tokentext)\n",
    "                print('测试集预处理：分词...')\n",
    "                sequence_test = tokenizer.texts_to_sequences(word_tokenized_corpus)\n",
    "                sequence_test = tf.keras.preprocessing.sequence.pad_sequences(sequence_test, maxlen=args.embedding_size)\n",
    "\n",
    "                self.texts = sequence_test\n",
    "                self.labels = [-1 for i in range(len(test))]\n",
    "                np.save(args.process_testset_path, sequence_test)\n",
    "            else:\n",
    "                test_set = list(np.load(args.process_testset_path))\n",
    "                self.texts = test_set\n",
    "                self.labels = [-1 for i in range(len(test_set))]\n",
    "        else:\n",
    "            raise Exception(\"No file for split %s\" % self.split)\n",
    "\n",
    "        assert len(self.texts) == len(self.labels)\n",
    "\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "\n",
    "        _text = self.texts[index]\n",
    "        _label = self.labels[index]\n",
    "\n",
    "        sample = {'text': _text, 'label': _label}\n",
    "        return self.transform(sample)\n",
    "\n",
    "    def transform(self, sample):\n",
    "        text = sample['text']\n",
    "        label = sample['label']\n",
    "\n",
    "        text = np.array(text)\n",
    "        label = np.array(label)\n",
    "        text = torch.from_numpy(text).to(torch.int64).to(DEVICE)\n",
    "        label = torch.from_numpy(label).to(torch.int64).to(DEVICE)\n",
    "\n",
    "        return {'text': text, 'label': label}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 截断文本，通过测试，200的时候截断效果最佳\n",
    "first_n_words = 200\n",
    "def trim_string(x):\n",
    "    x =str(x)\n",
    "    x = x.split(maxsplit=first_n_words)\n",
    "    x = ' '.join(x[:first_n_words])\n",
    "    return x\n",
    "def make_data_loader(args):\n",
    "#     train = pd.read_csv(args.train_all, sep='\\t')\n",
    "    train = pd.read_csv(args.train_all)\n",
    "    if not os.path.exists(args.train_path):\n",
    "        label_id2cate = dict(enumerate(train.categories.unique()))\n",
    "        label_cate2id = {value: key for key, value in label_id2cate.items()}\n",
    "        with open(args.label_path, 'wb') as f:\n",
    "            pickle.dump(label_id2cate, f, pickle.HIGHEST_PROTOCOL)\n",
    "        train_x = pd.DataFrame(columns=['text', 'label'])\n",
    "        # 拼接title与abstract\n",
    "        train['text'] = train['title'] + ' ' + train['abstract']\n",
    "        train_x['label'] = train['categories'].map(label_cate2id)\n",
    "        train_x['text'] = train['text'].apply(trim_string)\n",
    "        train_x.to_csv(args.train_path, index=False)\n",
    "        test = pd.read_csv(args.testall_path, sep='\\t')\n",
    "        # 测试集预处理\n",
    "        test_x = pd.DataFrame(columns=['text'])\n",
    "        # 拼接title与abstract\n",
    "        test['text'] = test['title'] + ' ' + test['abstract']\n",
    "        test_x['text'] = test['text'].apply(trim_string)\n",
    "        test_x.to_csv(args.test_path,index=False)\n",
    "    else:\n",
    "        test_x = pd.read_csv(args.test_path)\n",
    "        train_x = pd.read_csv(args.train_path)\n",
    "        with open(args.label_path, 'rb') as f:\n",
    "            label_id2cate = pickle.load(f)\n",
    "    embedding_matrix_ft, tokenizer = build_word2vec(args, train)\n",
    "    train_set = PaperData(args, tokenizer=tokenizer,split='train')\n",
    "    test_set = PaperData(args,tokenizer = tokenizer, split='test')\n",
    "    test_loader = DataLoader(test_set, batch_size=args.batch_size, shuffle=False)\n",
    "    return embedding_matrix_ft, train_set, test_loader, label_id2cate\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 训练模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 初始网络\n",
    "def init_network(model, method='kaiming', exclude='embedding', seed=123):  # method='kaiming'\n",
    "    for name, w in model.named_parameters():\n",
    "        if exclude not in name:\n",
    "            if 'weight' in name:\n",
    "                if method == 'xavier':\n",
    "                    nn.init.xavier_normal_(w)\n",
    "                elif method == 'kaiming':\n",
    "                    nn.init.kaiming_normal_(w)\n",
    "                else:\n",
    "                    nn.init.normal_(w)\n",
    "            elif 'bias' in name:\n",
    "                nn.init.constant_(w, 0)\n",
    "\n",
    "# def train(args, model, train_iter,val_iter):\n",
    "def train(args, model, train_set, test_iter, label_id2cate):\n",
    "    k_fold = 10\n",
    "    predict_all = np.zeros([10000,39])#存储测试集的 预测结果\n",
    "    K = 3\n",
    "    for n in range(k_fold):\n",
    "        # K折划分\n",
    "        train_iter, val_iter = load_data_kfold(train_set, args.batch_size, k_fold, n)\n",
    "        model.train()\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=args.lr)\n",
    "        total_batch = 0\n",
    "        dev_best_acc = 0\n",
    "        loss_avg = []\n",
    "        for epoch in range(args.num_epochs):\n",
    "#             fgm=FGM(model)\n",
    "            print('Fold=[{}/{}] Epoch [{}/{}]'.format(n+1,k_fold,epoch + 1, args.num_epochs))\n",
    "#             pgd = PGD(model)\n",
    "            for i, data in enumerate(train_iter):\n",
    "                text = data[\"text\"]\n",
    "                label = data[\"label\"]\n",
    "                outputs = model(text)\n",
    "                model.zero_grad()\n",
    "                loss = F.cross_entropy(outputs, label)\n",
    "                loss_avg.append(loss.item())\n",
    "                loss.backward()\n",
    "                # FGM对抗训练\n",
    "#                 fgm.attack() # 在embedding上添加对抗扰动\n",
    "#                 loss_adv = F.cross_entropy(model(text), label)\n",
    "#                 loss_adv.backward() # 反向传播，并在正常的grad基础上，累加对抗训练的梯度\n",
    "#                 fgm.restore() # 恢复embedding参数\n",
    "                \n",
    "#                 pgd.backup_grad()\n",
    "#                 # 对抗训练\n",
    "#                 for t in range(K):\n",
    "#                     pgd.attack(is_first_attack=t) #embedding上添加对抗扰动, first attack时备份param.data\n",
    "#                     if t != K-1:\n",
    "#                         model.zero_grad()\n",
    "#                     else:\n",
    "#                         pgd.restore_grad()\n",
    "#                     loss_adv = F.cross_entropy(model(text), label)\n",
    "#                     loss_adv.backward() # 反向传播，并在正常的grad基础上，累加对抗训练的梯度\n",
    "#                     pgd.restore() # 恢复embedding参数\n",
    "                \n",
    "                optimizer.step()\n",
    "                total_batch += 1\n",
    "                y_true = label.data.cpu()\n",
    "                y_pred = torch.max(outputs.data, 1)[1].cpu()\n",
    "                train_acc = metrics.accuracy_score(y_true, y_pred)\n",
    "            dev_acc, dev_loss = evaluate(model, val_iter)\n",
    "\n",
    "            if dev_acc > dev_best_acc:\n",
    "                dev_best_acc = dev_acc\n",
    "                torch.save(model.state_dict(), args.save_path +'/' + args.model + '.ckpt')\n",
    "                # print(\"saved model, best acc on dev: %.4f\" % dev_acc)\n",
    "\n",
    "            msg = 'Iter:{0} train_loss: {1:.3} train_acc: {2:.2%} val_oss: {3:.2} val_cc: {4:.3%}'\n",
    "            print(msg.format(total_batch, np.mean(loss_avg),\n",
    "                            train_acc, dev_loss, dev_acc))\n",
    "            model.train()\n",
    "        \n",
    "        result = []\n",
    "        with torch.no_grad():\n",
    "            for i, data in enumerate(test_iter):\n",
    "                text = data[\"text\"]\n",
    "                outputs = model(text)\n",
    "                y_pred = outputs.data.cpu().numpy()\n",
    "                result.extend(y_pred)\n",
    "        predict_all += np.array(result)\n",
    "    avg_predict = predict_all/k_fold\n",
    "    predict_kfold(avg_predict, args.model, label_id2cate)\n",
    "\n",
    "# 验证\n",
    "def evaluate(model, val_iter):\n",
    "    model.eval()\n",
    "    loss_total = 0\n",
    "    y_preds = []\n",
    "    y_trues = []\n",
    "    with torch.no_grad():\n",
    "        for data in val_iter:\n",
    "            text = data[\"text\"]\n",
    "            label = data[\"label\"]\n",
    "            outputs = model(text)\n",
    "            loss = F.cross_entropy(outputs, label)\n",
    "            loss_total += loss\n",
    "            y_true = label.tolist()\n",
    "            y_pred = torch.max(outputs.data, 1)[1].cpu().tolist()\n",
    "            y_trues.extend(y_true)\n",
    "            y_preds.extend(y_pred)\n",
    "    acc = metrics.accuracy_score(y_trues, y_preds)\n",
    "    return acc, loss_total/len(val_iter)\n",
    "# 生成提交文件\n",
    "def predict_kfold(avg_predict, model_name, label_id2cate):\n",
    "\n",
    "    result = np.argmax(avg_predict, axis=1)\n",
    "    sub = pd.read_csv('./data/sample_submit.csv')\n",
    "    sub['categories'] = list(result)\n",
    "    sub['categories'] = sub['categories'].map(label_id2cate)\n",
    "    sub.to_csv('submit/submit_{}.csv'.format(model_name), index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "torch.manual_seed(1)\n",
    "torch.cuda.manual_seed_all(1)\n",
    "torch.backends.cudnn.deterministic = True  # 保证每次结果一样\n",
    "# 加载数据集\n",
    "embedding_matrix_ft, train_set, test_loader, label_id2cate = make_data_loader(args)\n",
    "pretrained_path = torch.FloatTensor(embedding_matrix_ft)\n",
    "print(\"加载完成\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if args.model == \"TextCNN\":  # 0.8076(epoch=8)\n",
    "    model = TextCNN(args, pretrained_path)\n",
    "elif args.model == \"FastText\":  # 0.8070 (epoch=20)\n",
    "    model = FastText(args, pretrained_path)\n",
    "elif args.model == \"TextRNN\":  \n",
    "    model = TextRNN(args, pretrained_path)\n",
    "elif args.model == \"DPCNN\":\n",
    "    model = DPCNN(args, pretrained_path)\n",
    "\n",
    "model.to(DEVICE)\n",
    "\n",
    "\n",
    "# 开始训练\n",
    "train(args, model, train_set, test_iter, label_id2cate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyMT7VyaPYl/6zHNflCmsO1U",
   "collapsed_sections": [],
   "mount_file_id": "14R1ij1tV0q9LB0THe8ch4URLIeQeilMy",
   "name": "3 Title and Abstract.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "interpreter": {
   "hash": "2fc9f0689f2f32664301ce51aaed3853cc1802bb7b4d4a74b41993575fbadbc0"
  },
  "kernelspec": {
   "display_name": "Python 3.6.13 64-bit ('tf2': conda)",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": ""
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "07f6384740a84964a15ce1830eb1a277": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_2a1278e57998487fb0f14bf7da679fdb",
       "IPY_MODEL_d1ed2c568f434b8cae0bc65533009a24"
      ],
      "layout": "IPY_MODEL_5552001e234f4dc69bb9bd49a96725c5"
     }
    },
    "1ae51bd9148348c0a51ca2ed432d7254": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_743536d2245542f6b3b977abd2805d35",
      "placeholder": "​",
      "style": "IPY_MODEL_47d5065ab4de472ebfb17993a3affee1",
      "value": " 232k/232k [00:00&lt;00:00, 3.39MB/s]"
     }
    },
    "1dd1fcfde2124fb682b2c469803d865d": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2a1278e57998487fb0f14bf7da679fdb": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "Downloading: 100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_1dd1fcfde2124fb682b2c469803d865d",
      "max": 433,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_72199be93db3439593521260a2eb7678",
      "value": 433
     }
    },
    "47d5065ab4de472ebfb17993a3affee1": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "5552001e234f4dc69bb9bd49a96725c5": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "5ecbf244927a4754a4dab51059e3146b": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_73bf3df789af46d8a6ded1229fa2f36c",
       "IPY_MODEL_1ae51bd9148348c0a51ca2ed432d7254"
      ],
      "layout": "IPY_MODEL_a9b47321039d46c7bb36edac178bcc43"
     }
    },
    "62a6af82a63e467692678c4f338d54cf": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_9fc9b40b50ec4d38a29fe577b794b999",
       "IPY_MODEL_d7de17cfea1941eda538ff13d77695dd"
      ],
      "layout": "IPY_MODEL_ddac2ab1cbb74470932c6cece0197c83"
     }
    },
    "6ec1fd5ba3194f8e91475c41c3056d59": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "72199be93db3439593521260a2eb7678": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "73bf3df789af46d8a6ded1229fa2f36c": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "Downloading: 100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a4ea8691600e4b1cb1e38624fe247045",
      "max": 231508,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_aad122836dfa4878bf3cb71614391343",
      "value": 231508
     }
    },
    "743536d2245542f6b3b977abd2805d35": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "74fb65700005498a849af9aa21ba31dc": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "8e6ca803fecf4527ae2df5961d0f6158": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "9fc9b40b50ec4d38a29fe577b794b999": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "Downloading: 100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a7549ed09dfb4c369dc5b554c0b3b761",
      "max": 440473133,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_8e6ca803fecf4527ae2df5961d0f6158",
      "value": 440473133
     }
    },
    "a4ea8691600e4b1cb1e38624fe247045": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a7549ed09dfb4c369dc5b554c0b3b761": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a9b47321039d46c7bb36edac178bcc43": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "aad122836dfa4878bf3cb71614391343": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "ade4af3f12a746c4a235a76ded69ad69": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c697a4f163f84920b253eced55a47cc0": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d1ed2c568f434b8cae0bc65533009a24": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_c697a4f163f84920b253eced55a47cc0",
      "placeholder": "​",
      "style": "IPY_MODEL_6ec1fd5ba3194f8e91475c41c3056d59",
      "value": " 433/433 [00:06&lt;00:00, 62.2B/s]"
     }
    },
    "d7de17cfea1941eda538ff13d77695dd": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ade4af3f12a746c4a235a76ded69ad69",
      "placeholder": "​",
      "style": "IPY_MODEL_74fb65700005498a849af9aa21ba31dc",
      "value": " 440M/440M [00:06&lt;00:00, 66.5MB/s]"
     }
    },
    "ddac2ab1cbb74470932c6cece0197c83": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}