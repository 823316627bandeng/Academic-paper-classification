{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install nltk\n",
    "# import nltk\n",
    "# nltk.download('wordnet')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "import re\n",
    "import os\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 加载数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载数据\n",
    "train = pd.read_csv('./data/train.csv', sep='\\t')\n",
    "train = train.sample(frac=1)  # 打乱顺序\n",
    "test = pd.read_csv('./data/test.csv', sep='\\t')\n",
    "print('Train shape:', train.shape)\n",
    "print('Test shape:', test.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 文本处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "import re\n",
    "def preprocess_text(document):\n",
    "    stemmer = WordNetLemmatizer()\n",
    "    # 删除符号\n",
    "    text = str(document)\n",
    "    text = text.replace(\"\\n\", ' ')\n",
    "    # 用单个空格替换多个空格\n",
    "    text = re.sub(r'\\s+', ' ', text, flags=re.I)\n",
    "\n",
    "    # 转换为小写\n",
    "    text = text.lower()\n",
    "\n",
    "    # 词形还原\n",
    "    tokens = text.split()\n",
    "    tokens = [stemmer.lemmatize(word) for word in tokens]\n",
    "\n",
    "    preprocessed_text = ' '.join(tokens)\n",
    "\n",
    "    return preprocessed_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train[\"title\"] = train[\"title\"].progress_apply(lambda x: preprocess_text(x))\n",
    "train[\"abstract\"] = train[\"abstract\"].progress_apply(lambda x: preprocess_text(x))\n",
    "test[\"title\"] = test[\"title\"].progress_apply(lambda x: preprocess_text(x))\n",
    "test[\"abstract\"] = test[\"abstract\"].progress_apply(lambda x: preprocess_text(x))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 训练集合成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_text_list = []\n",
    "for index, row in train.iterrows():\n",
    "    title = row[\"title\"]\n",
    "    abstract = row[\"abstract\"]\n",
    "    text = \"[CLS] \" + title + \" [SEP] \" + abstract + \" [SEP]\"\n",
    "    train_text_list.append(text)\n",
    "train['text'] = train_text_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_path = \"data/label_id2cate.pkl\"\n",
    "#将标签进行转换\n",
    "label_id2cate = dict(enumerate(train.categories.unique()))\n",
    "label_cate2id = {value: key for key, value in label_id2cate.items()}\n",
    "\n",
    "with open(label_path, 'wb') as f:\n",
    "    pickle.dump(label_id2cate, f, pickle.HIGHEST_PROTOCOL)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train['label'] = train['categories'].map(label_cate2id)\n",
    "train_data = pd.DataFrame(columns=[\"text\",\"label\"])\n",
    "train_data[\"text\"] = train[\"text\"]\n",
    "train_data[\"label\"] = train[\"label\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.to_csv('data/train_clean_data.csv', sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 测试集合成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_text_list = []\n",
    "for index, row in test.iterrows():\n",
    "    title = row[\"title\"]\n",
    "    abstract = row[\"abstract\"]\n",
    "    text = \"[CLS] \" + title + \" [SEP] \" + abstract + \" [SEP]\"\n",
    "    test_text_list.append(text)\n",
    "test['text'] = test_text_list\n",
    "test['label'] = [-1]*len(test) \n",
    "# 去除换行符\n",
    "test_data = test.drop(['paperid', 'title', 'abstract'], axis=1, inplace=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data.to_csv('data/p_data_5w/test_clean_data.csv', sep='\\t', index=False)\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "2fc9f0689f2f32664301ce51aaed3853cc1802bb7b4d4a74b41993575fbadbc0"
  },
  "kernelspec": {
   "display_name": "Python 3.6.13 64-bit ('tf2': conda)",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": ""
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}